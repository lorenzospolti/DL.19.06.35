{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzospolti/DL.19.06.35/blob/main/hotel_review_pipeline_optionB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "30f35a71",
      "metadata": {
        "id": "30f35a71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_url = \"https://raw.githubusercontent.com/lorenzospolti/DL.19.06.35/main/input_data.csv\"\n",
        "df = pd.read_csv(csv_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c934ca4d",
      "metadata": {
        "id": "c934ca4d"
      },
      "source": [
        "# Hotel Review Multi‑Task Pipeline\n",
        "**Author:** Lorenzo Spolti"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8d4eb7",
      "metadata": {
        "id": "3e8d4eb7"
      },
      "source": [
        "## 0  Load Data\n",
        "Replace the file path with your dataset. The CSV is assumed to have these columns:\n",
        "\n",
        "* `Review` — raw text\n",
        "* `Review_Type` — 1 = positive, 0 = negative\n",
        "* `Review_Score` — 1‑10 numeric score\n",
        "* `hotel_name`, `reviewer_nationality` — categorical\n",
        "* `hotel_number_reviews`, `review_date` — numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "bda76fd1",
      "metadata": {
        "id": "bda76fd1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(csv_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "720912ad",
      "metadata": {
        "id": "720912ad"
      },
      "source": [
        "\n",
        "## 1  Model Overview\n",
        "We follow the exact design described in **“Answer to the exam.rtf”**:\n",
        "\n",
        "1. **Pre‑trained lightweight Transformer** (BERT‑tiny) provides language features  \n",
        "2. **WordPiece tokenisation** is inherited from the same BERT model  \n",
        "3. **Two additional branches** on top of the pooled `[CLS]` representation:\n",
        "   * **Head A** – binary classifier (`sigmoid`) → review type  \n",
        "   * **Head B** – regression (`linear`) → review score  \n",
        "4. **Structured features** are exploited by a small MLP and concatenated to the pooled text vector\n",
        "\n",
        "All pre‑trained weights are **frozen** by default; only the new layers learn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000edfab",
      "metadata": {
        "id": "000edfab"
      },
      "source": [
        "\n",
        "## 2  Input Pre‑processing\n",
        "### 2 .1  WordPiece Tokeniser\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "```\n",
        "\n",
        "### 2 .2  Categorical → Embeddings  \n",
        "We factorise each categorical column and keep the integer codes.\n",
        "\n",
        "### 2 .3  Numeric → Standard Scaler  \n",
        "`hotel_number_reviews` and a **days‑since** version of `review_date` are z‑scored.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "39c4d2cd",
      "metadata": {
        "id": "39c4d2cd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "\n",
        "# --- categorical ---\n",
        "cat_cols = ['Hotel_Name', 'Reviewer_Nationality']\n",
        "cat_maps = {c: pd.factorize(df[c])[0] for c in cat_cols}\n",
        "cat_tensors = [torch.tensor(v, dtype=torch.long) for v in cat_maps.values()]\n",
        "\n",
        "# --- numeric ---\n",
        "# make 'Review_Date' a numeric (days since first date)\n",
        "df['Review_Date'] = pd.to_datetime(df['Review_Date'])\n",
        "df['days_since'] = (df['Review_Date'] - df['Review_Date'].min()).dt.days\n",
        "num_cols = ['Hotel_number_reviews', 'days_since']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "num_array = scaler.fit_transform(df[num_cols]).astype('float32')\n",
        "num_tensor = torch.tensor(num_array)\n",
        "\n",
        "# --- text ---\n",
        "# Add max_length and return_token_type_ids=False to truncate and prevent token_type_ids\n",
        "enc = tokenizer(df['Review'].tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt', return_token_type_ids=False)\n",
        "input_ids = enc['input_ids']\n",
        "attention = enc['attention_mask']\n",
        "# token_type_ids will not be returned by the tokenizer now\n",
        "\n",
        "# --- targets ---\n",
        "# Map 'Bad_review' to 0 and 'Good_review' to 1, fill any resulting NaNs, and convert to int64\n",
        "df['Review_Type'] = df['Review_Type'].map({'Bad_review': 0, 'Good_review': 1}).fillna(0).astype('int64')\n",
        "y = torch.tensor(df['Review_Type'].values, dtype=torch.float32)\n",
        "scores = torch.tensor(df['Review_Score'].values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc5b6e16",
      "metadata": {
        "id": "cc5b6e16"
      },
      "source": [
        "## 4  Dataset & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "485ab240",
      "metadata": {
        "id": "485ab240"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stack categorical codes into a single tensor [N, C]\n",
        "cats = torch.stack(cat_tensors, dim=1)\n",
        "# Numeric is [N, num_features]\n",
        "nums = num_tensor\n",
        "\n",
        "idx = torch.arange(len(df))\n",
        "train_idx, val_idx = train_test_split(idx, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "train_ds = TensorDataset(\n",
        "    input_ids[train_idx], attention[train_idx],\n",
        "    cats[train_idx], nums[train_idx],\n",
        "    y[train_idx], scores[train_idx]\n",
        ")\n",
        "val_ds = TensorDataset(\n",
        "    input_ids[val_idx], attention[val_idx],\n",
        "    cats[val_idx], nums[val_idx],\n",
        "    y[val_idx], scores[val_idx]\n",
        ")\n",
        "\n",
        "BATCH = 8 # Reduced batch size to try and prevent CUDA out of memory\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH*2, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c649d0b7",
      "metadata": {
        "id": "c649d0b7"
      },
      "source": [
        "\n",
        "## 5  Model Definition\n",
        "The diagram matches the RTF answer exactly:\n",
        "\n",
        "```\n",
        "Input text        →  Transformer  →  [CLS]\n",
        "Input categorical →  Embeddings –┐\n",
        "Input numerical   →  MLP         ├─ concat → Dense → ReLU →    HEAD A  (sigmoid)\n",
        "                                 └─────────┤\n",
        "                                           └→ Dense → Dropout → HEAD B (linear)\n",
        "```\n",
        "All Transformer layers are frozen; we only unfreeze **N** top layers when `UNFREEZE_TOP_N > 0`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "195787db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "195787db",
        "outputId": "2b0dd1df-6adb-4043-dc1e-f4c5dd8e2261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model instantiated. Hidden size: 128\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "import types\n",
        "\n",
        "class ReviewModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 pretrained='prajjwal1/bert-tiny',\n",
        "                 cat_cardinals=None,\n",
        "                 num_features=2,\n",
        "                 cat_dim=32,\n",
        "                 proj_dim=32,\n",
        "                 head_dim=128):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(pretrained)\n",
        "        self.hidden = self.text_encoder.config.hidden_size\n",
        "\n",
        "        # Store the number of hidden layers for unfreezing logic\n",
        "        self.num_backbone_layers = self.text_encoder.config.num_hidden_layers\n",
        "\n",
        "        # Freeze everything\n",
        "        for p in self.text_encoder.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        # Embeddings for categorical cols\n",
        "        self.cat_embeddings = nn.ModuleDict({\n",
        "            name: nn.Embedding(card, cat_dim)\n",
        "            for name, card in cat_cardinals.items()\n",
        "        })\n",
        "        total_cat = cat_dim * len(cat_cardinals)\n",
        "\n",
        "        # Projection for numeric\n",
        "        self.num_proj = nn.Sequential(\n",
        "            nn.Linear(num_features, proj_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        fused_dim = self.hidden + total_cat + proj_dim\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(fused_dim, head_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Heads\n",
        "        self.cls_head = nn.Sequential(nn.Linear(head_dim, 1))  # sigmoid later\n",
        "        self.reg_head = nn.Linear(head_dim, 1)\n",
        "\n",
        "        # backbone alias for training script (keep original config)\n",
        "        self.backbone = self.text_encoder\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, cats, nums):\n",
        "        # Explicitly create token_type_ids with zeros\n",
        "        token_type_ids = torch.zeros_like(input_ids)\n",
        "        out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled = out.pooler_output if hasattr(out, 'pooler_output') and out.pooler_output is not None else out.last_hidden_state[:,0]\n",
        "\n",
        "        cat_vecs = [self.cat_embeddings[name](cats[:, i]) for i, name in enumerate(self.cat_embeddings)]\n",
        "        cat_concat = torch.cat(cat_vecs, dim=-1)\n",
        "\n",
        "        num_vec = self.num_proj(nums)\n",
        "\n",
        "        x = torch.cat([pooled, cat_concat, num_vec], dim=-1)\n",
        "        h = self.shared(x)\n",
        "\n",
        "        logit = self.cls_head(h).squeeze(-1)   # BCEWithLogitsLoss will apply sigmoid\n",
        "        score = self.reg_head(h).squeeze(-1)\n",
        "        return logit, score\n",
        "\n",
        "# Instantiate\n",
        "cat_cardinals = {c: int(df[c].nunique()) for c in cat_cols}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ReviewModel(cat_cardinals=cat_cardinals).to(device)\n",
        "print('Model instantiated. Hidden size:', model.hidden)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Concise multi‑input / multi‑task hotel‑review pipeline\n",
        "──────────────────────────────────────────────────────\n",
        "Split into two clear blocks:\n",
        "1. **Training** – returns model + raw predictions\n",
        "2. **Evaluation** – consumes predictions & ground‑truth to compute\n",
        "   • accuracy, precision, recall, F1 (Head A)\n",
        "   • MSE + RMSE (Head B)\n",
        "\n",
        "Configurable via a small `cfg` dict.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import math, random\n",
        "from typing import Dict, Any, List, Tuple\n",
        "\n",
        "import numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn import metrics as skm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# ────────────────────────── Dataset ──────────────────────────\n",
        "class ReviewDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tok: AutoTokenizer, max_len: int = 128):\n",
        "        self.df, self.tok, self.max_len = df.reset_index(drop=True), tok, max_len\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        enc = self.tok(row.Review, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        # Keep cat_feats as Long tensors for embedding layers\n",
        "        item[\"cat\"]   = torch.tensor(np.array(row.cat_feats), dtype=torch.long)\n",
        "        item[\"num\"]   = torch.tensor(np.array(row.num_feats), dtype=torch.float32)\n",
        "        item[\"label\"] = torch.tensor(row.Review_Type, dtype=torch.float32)\n",
        "        item[\"score\"] = torch.tensor(row.Review_Score, dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "# ─────────────────────────── Model ───────────────────────────\n",
        "class MTModel(nn.Module):\n",
        "    def __init__(self, backbone: str, cat_cardinals: Dict[str, int], n_num: int, dropout: float, unfrozen: int, cat_dim: int = 32):\n",
        "        super().__init__()\n",
        "        self.tfm = AutoModel.from_pretrained(backbone)\n",
        "        for p in self.tfm.parameters(): p.requires_grad_(False)\n",
        "        if unfrozen>0 and hasattr(self.tfm, \"encoder\"):\n",
        "            for lyr in self.tfm.encoder.layer[-unfrozen:]:\n",
        "                for p in lyr.parameters(): p.requires_grad_(True)\n",
        "        dim = self.tfm.config.hidden_size\n",
        "\n",
        "        # Embeddings for categorical cols\n",
        "        self.cat_embeddings = nn.ModuleDict({\n",
        "            name: nn.Embedding(card, cat_dim)\n",
        "            for name, card in cat_cardinals.items()\n",
        "        })\n",
        "        total_cat_emb_dim = cat_dim * len(cat_cardinals)\n",
        "\n",
        "\n",
        "        self.fc_cat = nn.Linear(dim + total_cat_emb_dim, 128)\n",
        "        self.out_a  = nn.Linear(128, 1)\n",
        "        self.fc_num = nn.Linear(dim + n_num, 128)\n",
        "        self.drop   = nn.Dropout(dropout)\n",
        "        self.out_b  = nn.Linear(128, 1)\n",
        "        self.relu, self.sig = nn.ReLU(), nn.Sigmoid()\n",
        "        self.apply(lambda m: nn.init.kaiming_normal_(m.weight) if isinstance(m, nn.Linear) else None)\n",
        "\n",
        "    def forward(self, ids, mask, cat, num):\n",
        "        cls = self.tfm(input_ids=ids, attention_mask=mask).last_hidden_state[:,0]\n",
        "\n",
        "        # Process categorical features through embedding layers\n",
        "        cat_vecs = [self.cat_embeddings[name](cat[:, i]) for i, name in enumerate(self.cat_embeddings)]\n",
        "        cat_concat = torch.cat(cat_vecs, dim=-1)\n",
        "\n",
        "        # Head A (using concatenated text and categorical embeddings)\n",
        "        x_a  = self.relu(self.fc_cat(torch.cat([cls, cat_concat], 1)))\n",
        "        outA = self.sig(self.out_a(x_a)).squeeze(1)\n",
        "\n",
        "        # Head B (using concatenated text and numerical features)\n",
        "        x_b  = self.drop(self.relu(self.fc_num(torch.cat([cls, num], 1))))\n",
        "        outB = self.out_b(x_b).squeeze(1)\n",
        "        return outA, outB\n",
        "\n",
        "# ─────────────────────── Block 1 – Training ───────────────────\n",
        "\n",
        "def train_model(model: MTModel, train_dl: torch.utils.data.DataLoader, val_dl: torch.utils.data.DataLoader, cfg: Dict[str, Any]) -> Tuple[Dict[str, np.ndarray], MTModel]:\n",
        "    \"\"\"Train for *epochs* with early stopping; return predictions on *val_dl*.\"\"\"\n",
        "    opt_cls = torch.optim.SGD if cfg[\"optim\"]==\"SGD\" else torch.optim.AdamW\n",
        "    opt = opt_cls(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n",
        "    best, patience = math.inf, 0\n",
        "    for _ in range(cfg[\"epochs\"]):\n",
        "        # –– training loop\n",
        "        model.train()\n",
        "        for b in train_dl:\n",
        "            opt.zero_grad()\n",
        "            logits, reg = model(b[\"input_ids\"].to(device), b[\"attention_mask\"].to(device), b[\"cat\"].to(device), b[\"num\"].to(device))\n",
        "            loss = cfg[\"lambda_reg\"]*F.binary_cross_entropy(logits, b[\"label\"].to(device)) + F.mse_loss(reg, b[\"score\"].to(device))\n",
        "            loss.backward()\n",
        "            if cfg[\"clip\"]: torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"clip\"])\n",
        "            opt.step()\n",
        "        # –– validation loss for early‑stop\n",
        "        model.eval(); val_loss=[]\n",
        "        with torch.no_grad():\n",
        "            for b in val_dl:\n",
        "                l, r = model(b[\"input_ids\"].to(device), b[\"attention_mask\"].to(device), b[\"cat\"].to(device), b[\"num\"].to(device))\n",
        "                vloss = cfg[\"lambda_reg\"]*F.binary_cross_entropy(l, b[\"label\"].to(device)) + F.mse_loss(r, b[\"score\"].to(device))\n",
        "                val_loss.append(vloss.item())\n",
        "        cur = np.mean(val_loss)\n",
        "        if cur < best: best, patience = cur, 0\n",
        "        else: patience += 1\n",
        "        if patience >= cfg[\"early\"]: break\n",
        "    # –– collect predictions on val set\n",
        "    logits_all, reg_all, lab_all, score_all = [], [], [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for b in val_dl:\n",
        "            l, r = model(b[\"input_ids\"].to(device), b[\"attention_mask\"].to(device), b[\"cat\"].to(device), b[\"num\"].to(device))\n",
        "            logits_all.extend(l.cpu().numpy());   lab_all.extend(b[\"label\"].numpy())\n",
        "            reg_all.extend(r.cpu().numpy());      score_all.extend(b[\"score\"].numpy())\n",
        "    preds = {\n",
        "        \"cls_pred\": np.array(logits_all),\n",
        "        \"cls_true\": np.array(lab_all),\n",
        "        \"reg_pred\": np.array(reg_all),\n",
        "        \"reg_true\": np.array(score_all),\n",
        "    }\n",
        "    return preds, model\n",
        "\n",
        "# ─────────────────────── Block 2 – Evaluation ─────────────────\n",
        "\n",
        "def evaluate(preds: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
        "    y_hat = (preds[\"cls_pred\"] > 0.5).astype(int)\n",
        "    metrics = {\n",
        "        \"accuracy\":  skm.accuracy_score(preds[\"cls_true\"], y_hat),\n",
        "        \"precision\": skm.precision_score(preds[\"cls_true\"], y_hat, zero_division=0),\n",
        "        \"recall\":    skm.recall_score(preds[\"cls_true\"], y_hat, zero_division=0),\n",
        "        \"f1\":        skm.f1_score(preds[\"cls_true\"], y_hat, zero_division=0),\n",
        "        \"mse\":       skm.mean_squared_error(preds[\"reg_true\"], preds[\"reg_pred\"]),\n",
        "        \"rmse\":      math.sqrt(skm.mean_squared_error(preds[\"reg_true\"], preds[\"reg_pred\"])),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# ───────────────────────── Cross‑val 5× ───────────────────────\n",
        "\n",
        "def cross_validate(df: pd.DataFrame, cfg: Dict[str, Any]):\n",
        "    tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    # Prepare cat_feats and num_feats columns\n",
        "    cat_cols = ['Hotel_Name', 'Reviewer_Nationality']\n",
        "    num_cols = ['Hotel_number_reviews', 'days_since']\n",
        "\n",
        "    # Ensure cat_feats and num_feats are stored as lists of numerical values\n",
        "    # Use factorize directly for cat_feats to get integer codes\n",
        "    for col in cat_cols:\n",
        "        df[col], _ = pd.factorize(df[col])\n",
        "    df['cat_feats'] = df[cat_cols].values.tolist()\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    df['num_feats'] = list(scaler.fit_transform(df[num_cols]).astype('float32'))\n",
        "\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    results: List[Dict[str,float]] = []\n",
        "    # Assuming 'hotel_id' is a column in your dataframe for grouping.\n",
        "    # If not, you might need to create one or use a different grouping strategy.\n",
        "    # For now, I'll use 'Hotel_Name' as a proxy for grouping if 'hotel_id' is not available.\n",
        "    group_col = 'hotel_id' if 'hotel_id' in df.columns else 'Hotel_Name'\n",
        "\n",
        "    # Get categorical cardinalities for the model\n",
        "    cat_cardinals = {col: df[col].nunique() for col in cat_cols}\n",
        "\n",
        "    for idx, (tr, va) in enumerate(gkf.split(df, groups=df[group_col])):\n",
        "        # Get the number of numerical features from the prepared data\n",
        "        n_num = len(df['num_feats'].iloc[0])\n",
        "\n",
        "        # Pass cat_cardinals to the model\n",
        "        mdl = MTModel(\"distilbert-base-uncased\", cat_cardinals, n_num, cfg[\"dropout\"], cfg[\"unfreeze\"]).to(device)\n",
        "        tr_dl = torch.utils.data.DataLoader(ReviewDS(df.iloc[tr], tok), batch_size=cfg[\"bs\"], shuffle=True)\n",
        "        # Set validation batch size to be the same as training batch size\n",
        "        va_dl = torch.utils.data.DataLoader(ReviewDS(df.iloc[va], tok), batch_size=cfg[\"bs\"])\n",
        "        preds, _ = train_model(mdl, tr_dl, va_dl, cfg)\n",
        "        res = evaluate(preds)\n",
        "        results.append(res)\n",
        "        print(f\"Fold {idx+1}:\", {k:f\"{v:.4f}\" for k,v in res.items()})\n",
        "    # aggregate\n",
        "    mean = {k: np.mean([r[k] for r in results]) for k in results[0]}\n",
        "    std  = {k: np.std( [r[k] for r in results]) for k in results[0]}\n",
        "    print(\"\\nMean ± SD across folds:\")\n",
        "    for k in mean: print(f\"{k}: {mean[k]:.4f} ± {std[k]:.4f}\")\n",
        "\n",
        "# ────────────────── Example config & runner ───────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = dict(lr=2e-5, wd=1e-2, dropout=0.2, unfreeze=0, bs=16, optim=\"AdamW\",\n",
        "               lambda_reg=1.0, clip=1.0, epochs=5, early=2)\n",
        "    cross_validate(df, cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eERqXjmWFNhb",
        "outputId": "4d9ee03e-7015-4d4a-a683-e1535275d6a5"
      },
      "id": "eERqXjmWFNhb",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: {'accuracy': '0.8762', 'precision': '0.8293', 'recall': '0.9312', 'f1': '0.8773', 'mse': '2.4863', 'rmse': '1.5768'}\n",
            "Fold 2: {'accuracy': '0.8795', 'precision': '0.8539', 'recall': '0.9257', 'f1': '0.8884', 'mse': '2.2467', 'rmse': '1.4989'}\n",
            "Fold 3: {'accuracy': '0.8696', 'precision': '0.8661', 'recall': '0.8861', 'f1': '0.8760', 'mse': '2.2088', 'rmse': '1.4862'}\n",
            "Fold 4: {'accuracy': '0.8729', 'precision': '0.8406', 'recall': '0.9079', 'f1': '0.8729', 'mse': '2.1303', 'rmse': '1.4595'}\n",
            "Fold 5: {'accuracy': '0.8889', 'precision': '0.8583', 'recall': '0.9347', 'f1': '0.8948', 'mse': '2.4323', 'rmse': '1.5596'}\n",
            "\n",
            "Mean ± SD across folds:\n",
            "accuracy: 0.8774 ± 0.0066\n",
            "precision: 0.8496 ± 0.0131\n",
            "recall: 0.9171 ± 0.0181\n",
            "f1: 0.8819 ± 0.0083\n",
            "mse: 2.3009 ± 0.1358\n",
            "rmse: 1.5162 ± 0.0446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb51110",
      "metadata": {
        "id": "fdb51110"
      },
      "source": [
        "## 6  Training"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}