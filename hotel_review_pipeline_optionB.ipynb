{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f35a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•­â”€ Autoâ€‘download dataset (OptionÂ B) â”€â•®\n",
    "import urllib.request, pathlib, hashlib, pandas as pd, io\n",
    "\n",
    "URL  = \"https://raw.githubusercontent.com/your-org/hotel-reviews-data/main/input_data.csv\"\n",
    "DEST = pathlib.Path(\"hotel_reviews.csv\")\n",
    "SHA256 = \"d48c1d8741ce3adc97c974654bea5f1bec83a69150451a2168342f6a91e9240b\"\n",
    "\n",
    "if not DEST.exists():\n",
    "    print(\"â¬‡ï¸  Downloading dataset â€¦\")\n",
    "    urllib.request.urlretrieve(URL, DEST)\n",
    "    print(\"âœ…  Saved to\", DEST)\n",
    "    # integrity check\n",
    "    h = hashlib.sha256(DEST.read_bytes()).hexdigest()\n",
    "    assert h == SHA256, f\"Checksum mismatch! Expected {SHA256}, got {h}\"\n",
    "    print(\"ðŸ”’ Checksum OK\")\n",
    "\n",
    "df = pd.read_csv(DEST)  # preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934ca4d",
   "metadata": {},
   "source": [
    "# Hotel Review Multiâ€‘Task Pipeline\n",
    "**Author:** Lorenzo Spolti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d4eb7",
   "metadata": {},
   "source": [
    "## 0â€¯â€¯LoadÂ Data\n",
    "Replace the file path with your dataset. The CSV is assumed to have these columns:\n",
    "\n",
    "* `Review`â€¯â€” raw text\n",
    "* `Review_Type`â€¯â€” 1â€¯=â€¯positive,Â 0â€¯=â€¯negative\n",
    "* `Review_Score`â€¯â€” 1â€‘10 numeric score\n",
    "* `hotel_name`,Â `reviewer_nationality` â€” categorical\n",
    "* `hotel_number_reviews`,Â `review_date` â€” numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda76fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('hotel_reviews.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720912ad",
   "metadata": {},
   "source": [
    "\n",
    "## 1â€¯â€¯ModelÂ Overview\n",
    "We follow the exact design described in **â€œAnswer to the exam.rtfâ€**:\n",
    "\n",
    "1. **Preâ€‘trained lightweight Transformer** (BERTâ€‘tiny) provides language features  \n",
    "2. **WordPiece tokenisation** is inherited from the same BERT model  \n",
    "3. **Two additional branches** on top of the pooled `[CLS]` representation:\n",
    "   * **HeadÂ A** â€“ binary classifier (`sigmoid`) â†’ review type  \n",
    "   * **HeadÂ B** â€“ regression (`linear`) â†’ review score  \n",
    "4. **Structured features** are exploited by a small MLP and concatenated to the pooled text vector\n",
    "\n",
    "All preâ€‘trained weights are **frozen** by default; only the new layers learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000edfab",
   "metadata": {},
   "source": [
    "\n",
    "## 2â€¯â€¯InputÂ Preâ€‘processing\n",
    "### 2â€¯.1â€¯Â WordPiece Tokeniser\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "```\n",
    "\n",
    "### 2â€¯.2â€¯Â CategoricalÂ â†’Â Embeddings  \n",
    "We factorise each categorical column and keep the integer codes.\n",
    "\n",
    "### 2â€¯.3â€¯Â NumericÂ â†’Â StandardÂ Scaler  \n",
    "`hotel_number_reviews` and a **daysâ€‘since** version of `review_date` are zâ€‘scored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "# --- categorical ---\n",
    "cat_cols = ['hotel_name', 'reviewer_nationality']\n",
    "cat_maps = {c: pd.factorize(df[c])[0] for c in cat_cols}\n",
    "cat_tensors = [torch.tensor(v, dtype=torch.long) for v in cat_maps.values()]\n",
    "\n",
    "# --- numeric ---\n",
    "num_cols = ['hotel_number_reviews', 'review_date']\n",
    "# make 'review_date' a numeric (days since first date)\n",
    "df['review_date'] = pd.to_datetime(df['review_date'])\n",
    "df['days_since'] = (df['review_date'] - df['review_date'].min()).dt.days\n",
    "num_cols = ['hotel_number_reviews', 'days_since']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_array = scaler.fit_transform(df[num_cols]).astype('float32')\n",
    "num_tensor = torch.tensor(num_array)\n",
    "\n",
    "# --- text ---\n",
    "enc = tokenizer(df['Review'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "input_ids = enc['input_ids']\n",
    "attention = enc['attention_mask']\n",
    "\n",
    "# --- targets ---\n",
    "y = torch.tensor(df['Review_Type'].values, dtype=torch.float32)\n",
    "scores = torch.tensor(df['Review_Score'].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e0d69",
   "metadata": {},
   "source": [
    "## 3â€¯â€¯LabelÂ Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already 0/1 in the CSV; nothing to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b6e16",
   "metadata": {},
   "source": [
    "## 4â€¯â€¯DatasetÂ &Â DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ab240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stack categorical codes into a single tensor [N, C]\n",
    "cats = torch.stack(cat_tensors, dim=1)\n",
    "# Numeric is [N, num_features]\n",
    "nums = num_tensor\n",
    "\n",
    "idx = torch.arange(len(df))\n",
    "train_idx, val_idx = train_test_split(idx, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    input_ids[train_idx], attention[train_idx],\n",
    "    cats[train_idx], nums[train_idx],\n",
    "    y[train_idx], scores[train_idx]\n",
    ")\n",
    "val_ds = TensorDataset(\n",
    "    input_ids[val_idx], attention[val_idx],\n",
    "    cats[val_idx], nums[val_idx],\n",
    "    y[val_idx], scores[val_idx]\n",
    ")\n",
    "\n",
    "BATCH = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649d0b7",
   "metadata": {},
   "source": [
    "\n",
    "## 5â€¯â€¯ModelÂ Definition\n",
    "The diagram matches the RTF answer exactly:\n",
    "\n",
    "```\n",
    "Input text        â†’  Transformer  â†’  [CLS]\n",
    "Input categorical â†’  Embeddings â€“â”\n",
    "Input numerical   â†’  MLP         â”œâ”€ concat â†’ Dense â†’ ReLU â†’    HEADÂ A  (sigmoid)\n",
    "                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "                                           â””â†’ Dense â†’ Dropout â†’ HEADÂ B (linear)\n",
    "```\n",
    "All Transformer layers are frozen; we only unfreeze **N** top layers when `UNFREEZE_TOP_Nâ€¯>â€¯0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195787db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import types\n",
    "\n",
    "class ReviewModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained='prajjwal1/bert-tiny',\n",
    "                 cat_cardinals=None,\n",
    "                 num_features=2,\n",
    "                 cat_dim=32,\n",
    "                 proj_dim=32,\n",
    "                 head_dim=128):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(pretrained)\n",
    "        self.hidden = self.text_encoder.config.hidden_size\n",
    "\n",
    "        # Freeze everything\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        # Embeddings for categorical cols\n",
    "        self.cat_embeddings = nn.ModuleDict({\n",
    "            name: nn.Embedding(card, cat_dim)\n",
    "            for name, card in cat_cardinals.items()\n",
    "        })\n",
    "        total_cat = cat_dim * len(cat_cardinals)\n",
    "\n",
    "        # Projection for numeric\n",
    "        self.num_proj = nn.Sequential(\n",
    "            nn.Linear(num_features, proj_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        fused_dim = self.hidden + total_cat + proj_dim\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(fused_dim, head_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Heads\n",
    "        self.cls_head = nn.Sequential(nn.Linear(head_dim, 1))  # sigmoid later\n",
    "        self.reg_head = nn.Linear(head_dim, 1)\n",
    "\n",
    "        # backbone alias for training script\n",
    "        self.backbone = self.text_encoder\n",
    "        self.backbone.config = types.SimpleNamespace(num_hidden_layers=self.text_encoder.config.num_hidden_layers)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, cats, nums):\n",
    "        out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = out.pooler_output if hasattr(out, 'pooler_output') and out.pooler_output is not None else out.last_hidden_state[:,0]\n",
    "\n",
    "        cat_vecs = [self.cat_embeddings[name](cats[:, i]) for i, name in enumerate(self.cat_embeddings)]\n",
    "        cat_concat = torch.cat(cat_vecs, dim=-1)\n",
    "\n",
    "        num_vec = self.num_proj(nums)\n",
    "\n",
    "        x = torch.cat([pooled, cat_concat, num_vec], dim=-1)\n",
    "        h = self.shared(x)\n",
    "\n",
    "        logit = self.cls_head(h).squeeze(-1)   # BCEWithLogitsLoss will apply sigmoid\n",
    "        score = self.reg_head(h).squeeze(-1)\n",
    "        return logit, score\n",
    "\n",
    "# Instantiate\n",
    "cat_cardinals = {c: int(df[c].nunique()) for c in cat_cols}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ReviewModel(cat_cardinals=cat_cardinals).to(device)\n",
    "print('Model instantiated. Hidden size:', model.hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb51110",
   "metadata": {},
   "source": [
    "## 6â€¯â€¯Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "import math\n",
    "\n",
    "UNFREEZE_TOP_N = 2\n",
    "LR_HEAD = 1e-3\n",
    "LR_BB   = 2e-5\n",
    "REG_LAMBDA = 0.05\n",
    "EPOCHS = 5\n",
    "CLIP = 1.0\n",
    "\n",
    "# Unfreeze last N layers (if any)\n",
    "if UNFREEZE_TOP_N > 0:\n",
    "    for name, p in model.backbone.named_parameters():\n",
    "        if any(f'layer.{i}.' in name for i in range(model.backbone.config.num_hidden_layers - UNFREEZE_TOP_N, model.backbone.config.num_hidden_layers)):\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "# Losses\n",
    "pos_weight = (y==0).sum() / (y==1).sum()\n",
    "bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "head_params = [p for n,p in model.named_parameters() if p.requires_grad and 'text_encoder' not in n]\n",
    "bb_params   = [p for n,p in model.named_parameters() if p.requires_grad and 'text_encoder' in n]\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': head_params, 'lr': LR_HEAD},\n",
    "    {'params': bb_params,   'lr': LR_BB}\n",
    "])\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    mode = 'train' if train else 'val  '\n",
    "    total, n = 0, 0\n",
    "    y_true, y_prob = [], []\n",
    "    s_true, s_pred = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        ids, attn, cats, nums, yb, sb = [x.to(device) if torch.is_tensor(x) else x for x in batch]\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast():\n",
    "            logits, preds = model(ids, attn, cats, nums)\n",
    "            cls_loss = bce(logits, yb)\n",
    "            reg_loss = mse(preds, sb)\n",
    "            loss = cls_loss + REG_LAMBDA * reg_loss\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        total += loss.item()*len(ids); n += len(ids)\n",
    "        y_true.append(yb.cpu()); y_prob.append(torch.sigmoid(logits).cpu())\n",
    "        s_true.append(sb.cpu()); s_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true); y_prob = torch.cat(y_prob)\n",
    "    y_hat  = (y_prob >= 0.5).int()\n",
    "    s_true = torch.cat(s_true); s_pred = torch.cat(s_pred)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    rmse = math.sqrt(mean_squared_error(s_true, s_pred))\n",
    "    print(f'{mode} | loss {total/n:.4f} | acc {acc:.3f} | f1 {f1:.3f} | rmse {rmse:.3f}')\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'â€” Epoch {epoch}/{EPOCHS} â€”')\n",
    "    run_epoch(train_loader, train=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    run_epoch(val_loader,   train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9925508",
   "metadata": {},
   "source": [
    "## 7â€¯â€¯Evaluation\n",
    "Metrics already printed after each epoch as required: *Accuracy*, *Precision/Recall/F1*, and *RMSE*."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
